# KT Practical Exam â€“ End-to-End MLOps Pipeline

This project demonstrates a complete MLOps workflow including:

- Data Version Control (DVC)
- Model training with MLflow tracking
- FastAPI model serving
- Email alerts on failure
- Docker containerization
- Docker Compose orchestration
- GitHub Actions CI pipeline

---

# ğŸš€ Project Overview

We train a simple Linear Regression model to predict house prices based on:

- sqft
- bedrooms
- bathrooms

The model is:
- Versioned using DVC
- Tracked using MLflow
- Served using FastAPI
- Containerized using Docker
- Tested using GitHub Actions

---

# ğŸ›  Tech Stack

- Python 3.10
- FastAPI
- Scikit-learn
- MLflow
- DVC
- Docker
- GitHub Actions
- Gmail SMTP (for email alerts)

---

# ğŸ“‚ Project Structure

```
SET 6/
â”‚
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ model.pkl
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ data.csv
â”‚
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ params.yml
â”‚
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ dvc.yaml
â”œâ”€â”€ dvc.lock
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â””â”€â”€ README.md
```

---

# âš™ï¸ LOCAL SETUP INSTRUCTIONS

## 1ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
```

Activate environment:

Windows:
```bash
venv\Scripts\activate
```

Mac/Linux:
```bash
source venv/bin/activate
```

---

## 2ï¸âƒ£ Install Dependencies

```bash
pip install -r api/requirements.txt
pip install mlflow dvc pyyaml
```

---

# ğŸ“Š DVC SETUP

Initialize DVC (only first time):

```bash
dvc init
```

Track dataset:

```bash
dvc add data/data.csv
git add data/data.csv.dvc .gitignore
git commit -m "Track dataset with DVC"
```

Run pipeline:

```bash
dvc repro
```

This will:
- Train the model
- Save model.pkl
- Create dvc.lock file

---

# ğŸ“ˆ MLflow Tracking

Start MLflow UI:

```bash
mlflow ui
```

Open in browser:

```
http://127.0.0.1:5000
```

You will see:
- Logged parameters
- Metrics (MAE)
- Experiment history

---

# ğŸ§  Train Model Manually (Without DVC)

```bash
python ml/train.py
```

This will create:

```
api/model.pkl
```

---

# ğŸŒ Run FastAPI Locally

Navigate to api folder:

```bash
cd api
uvicorn app:app --reload
```

Open in browser:

```
http://127.0.0.1:8000
```

---

# ğŸ”® Test Prediction Endpoint

POST request to:

```
http://127.0.0.1:8000/predict
```

JSON Body:

```json
{
  "sqft": 2000,
  "bedrooms": 3,
  "bathrooms": 2
}
```

---

# ğŸ“§ Email Alert Setup

Create a `.env` file in the project root:

```
EMAIL_USER=yourgmail@gmail.com
EMAIL_PASS=your_app_password
EMAIL_RECEIVER=receiver@gmail.com
```

Important:
- Use Gmail App Password
- Do NOT use your real Gmail password

If invalid payload is sent, email alert will be triggered automatically.

Example failure payload:

```json
{
  "sqft": 2000
}
```

---

# ğŸ³ Docker Build

Build Docker image:

```bash
docker build -t kt-ml-app ./docker
```

Run container:

```bash
docker run -p 8000:8000 --env-file .env kt-ml-app
```

---

# ğŸ³ Docker Compose

Run full stack:

```bash
docker compose up --build
```

Check running containers:

```bash
docker ps
```

---

# ğŸ” GitHub Actions CI/CD

On every push to main branch:

- Install dependencies
- Run training script
- Build Docker image

To manually trigger:

```bash
git commit --allow-empty -m "Trigger CI"
git push
```

Check status in:

GitHub â†’ Actions Tab

---

# ğŸ§ª How to Test Email Failure

Send incorrect payload:

```json
{
  "sqft": 1000
}
```

You should receive an alert email.

---

# ğŸ§¹ Troubleshooting

### DVC Validation Error
Ensure:
- No incorrect `.dvc` files inside ml folder
- Use `dvc.yaml` in root

### Model Not Found
Run:
```bash
python ml/train.py
```

### Email Not Working
- Check `.env` file
- Ensure Gmail App Password is correct
- Enable App Passwords in Google Account

---

# ğŸ¯ Evaluation Checklist

- DVC tracking working
- MLflow logging parameters
- FastAPI prediction working
- Email alert on failure
- Docker container running
- GitHub Actions successful

---

# ğŸ Conclusion

This project demonstrates a complete end-to-end MLOps pipeline including:

Data Versioning â†’ Model Training â†’ Experiment Tracking â†’ API Serving â†’ Containerization â†’ CI/CD Automation.

